// Copyright 2025 The Go Authors. All rights reserved.
// Use of this source code is governed by a BSD-style
// license that can be found in the LICENSE file.

//go:build !purego

#include "textflag.h"

#define ROT64_AVX512(reg, amount) \
	VPROLQ	$amount, reg, reg

#define CHI_AVX512(base) \
	VMOVDQU	Y0, Y10; \
	VMOVDQU	Y1, Y11; \
	VPTERNLOGQ $0xD2, Y2, Y1, Y0; \
	VMOVDQU	Y0, (base+0)*32(R9); \
	VPTERNLOGQ $0xD2, Y3, Y2, Y1; \
	VMOVDQU	Y1, (base+1)*32(R9); \
	VPTERNLOGQ $0xD2, Y4, Y3, Y2; \
	VMOVDQU	Y2, (base+2)*32(R9); \
	VPTERNLOGQ $0xD2, Y10, Y4, Y3; \
	VMOVDQU	Y3, (base+3)*32(R9); \
	VPTERNLOGQ $0xD2, Y11, Y10, Y4; \
	VMOVDQU	Y4, (base+4)*32(R9)

#define CHI_IOTA_AVX512(base) \
	VMOVDQU	Y0, Y10; \
	VMOVDQU	Y1, Y11; \
	VPTERNLOGQ $0xD2, Y2, Y1, Y0; \
	VPXOR	Y15, Y0, Y0; \
	VMOVDQU	Y0, (base+0)*32(R9); \
	VPTERNLOGQ $0xD2, Y3, Y2, Y1; \
	VMOVDQU	Y1, (base+1)*32(R9); \
	VPTERNLOGQ $0xD2, Y4, Y3, Y2; \
	VMOVDQU	Y2, (base+2)*32(R9); \
	VPTERNLOGQ $0xD2, Y10, Y4, Y3; \
	VMOVDQU	Y3, (base+3)*32(R9); \
	VPTERNLOGQ $0xD2, Y11, Y10, Y4; \
	VMOVDQU	Y4, (base+4)*32(R9)

TEXT ·p1600x4AVX512(SB), $1600-32
	MOVQ	a+0(FP), DI
	MOVQ	b+8(FP), SI
	MOVQ	c+16(FP), DX
	MOVQ	d+24(FP), CX
	VMOVQ	0(DI), X0
	VMOVQ	0(SI), X1
	VPUNPCKLQDQ	X1, X0, X0
	VMOVQ	0(DX), X2
	VMOVQ	0(CX), X3
	VPUNPCKLQDQ	X3, X2, X2
	VINSERTI128	$1, X2, Y0, Y0
	VMOVDQU	Y0, 0*32(SP)
	VMOVQ	8(DI), X0
	VMOVQ	8(SI), X1
	VPUNPCKLQDQ	X1, X0, X0
	VMOVQ	8(DX), X2
	VMOVQ	8(CX), X3
	VPUNPCKLQDQ	X3, X2, X2
	VINSERTI128	$1, X2, Y0, Y0
	VMOVDQU	Y0, 1*32(SP)
	VMOVQ	16(DI), X0
	VMOVQ	16(SI), X1
	VPUNPCKLQDQ	X1, X0, X0
	VMOVQ	16(DX), X2
	VMOVQ	16(CX), X3
	VPUNPCKLQDQ	X3, X2, X2
	VINSERTI128	$1, X2, Y0, Y0
	VMOVDQU	Y0, 2*32(SP)
	VMOVQ	24(DI), X0
	VMOVQ	24(SI), X1
	VPUNPCKLQDQ	X1, X0, X0
	VMOVQ	24(DX), X2
	VMOVQ	24(CX), X3
	VPUNPCKLQDQ	X3, X2, X2
	VINSERTI128	$1, X2, Y0, Y0
	VMOVDQU	Y0, 3*32(SP)
	VMOVQ	32(DI), X0
	VMOVQ	32(SI), X1
	VPUNPCKLQDQ	X1, X0, X0
	VMOVQ	32(DX), X2
	VMOVQ	32(CX), X3
	VPUNPCKLQDQ	X3, X2, X2
	VINSERTI128	$1, X2, Y0, Y0
	VMOVDQU	Y0, 4*32(SP)
	VMOVQ	40(DI), X0
	VMOVQ	40(SI), X1
	VPUNPCKLQDQ	X1, X0, X0
	VMOVQ	40(DX), X2
	VMOVQ	40(CX), X3
	VPUNPCKLQDQ	X3, X2, X2
	VINSERTI128	$1, X2, Y0, Y0
	VMOVDQU	Y0, 5*32(SP)
	VMOVQ	48(DI), X0
	VMOVQ	48(SI), X1
	VPUNPCKLQDQ	X1, X0, X0
	VMOVQ	48(DX), X2
	VMOVQ	48(CX), X3
	VPUNPCKLQDQ	X3, X2, X2
	VINSERTI128	$1, X2, Y0, Y0
	VMOVDQU	Y0, 6*32(SP)
	VMOVQ	56(DI), X0
	VMOVQ	56(SI), X1
	VPUNPCKLQDQ	X1, X0, X0
	VMOVQ	56(DX), X2
	VMOVQ	56(CX), X3
	VPUNPCKLQDQ	X3, X2, X2
	VINSERTI128	$1, X2, Y0, Y0
	VMOVDQU	Y0, 7*32(SP)
	VMOVQ	64(DI), X0
	VMOVQ	64(SI), X1
	VPUNPCKLQDQ	X1, X0, X0
	VMOVQ	64(DX), X2
	VMOVQ	64(CX), X3
	VPUNPCKLQDQ	X3, X2, X2
	VINSERTI128	$1, X2, Y0, Y0
	VMOVDQU	Y0, 8*32(SP)
	VMOVQ	72(DI), X0
	VMOVQ	72(SI), X1
	VPUNPCKLQDQ	X1, X0, X0
	VMOVQ	72(DX), X2
	VMOVQ	72(CX), X3
	VPUNPCKLQDQ	X3, X2, X2
	VINSERTI128	$1, X2, Y0, Y0
	VMOVDQU	Y0, 9*32(SP)
	VMOVQ	80(DI), X0
	VMOVQ	80(SI), X1
	VPUNPCKLQDQ	X1, X0, X0
	VMOVQ	80(DX), X2
	VMOVQ	80(CX), X3
	VPUNPCKLQDQ	X3, X2, X2
	VINSERTI128	$1, X2, Y0, Y0
	VMOVDQU	Y0, 10*32(SP)
	VMOVQ	88(DI), X0
	VMOVQ	88(SI), X1
	VPUNPCKLQDQ	X1, X0, X0
	VMOVQ	88(DX), X2
	VMOVQ	88(CX), X3
	VPUNPCKLQDQ	X3, X2, X2
	VINSERTI128	$1, X2, Y0, Y0
	VMOVDQU	Y0, 11*32(SP)
	VMOVQ	96(DI), X0
	VMOVQ	96(SI), X1
	VPUNPCKLQDQ	X1, X0, X0
	VMOVQ	96(DX), X2
	VMOVQ	96(CX), X3
	VPUNPCKLQDQ	X3, X2, X2
	VINSERTI128	$1, X2, Y0, Y0
	VMOVDQU	Y0, 12*32(SP)
	VMOVQ	104(DI), X0
	VMOVQ	104(SI), X1
	VPUNPCKLQDQ	X1, X0, X0
	VMOVQ	104(DX), X2
	VMOVQ	104(CX), X3
	VPUNPCKLQDQ	X3, X2, X2
	VINSERTI128	$1, X2, Y0, Y0
	VMOVDQU	Y0, 13*32(SP)
	VMOVQ	112(DI), X0
	VMOVQ	112(SI), X1
	VPUNPCKLQDQ	X1, X0, X0
	VMOVQ	112(DX), X2
	VMOVQ	112(CX), X3
	VPUNPCKLQDQ	X3, X2, X2
	VINSERTI128	$1, X2, Y0, Y0
	VMOVDQU	Y0, 14*32(SP)
	VMOVQ	120(DI), X0
	VMOVQ	120(SI), X1
	VPUNPCKLQDQ	X1, X0, X0
	VMOVQ	120(DX), X2
	VMOVQ	120(CX), X3
	VPUNPCKLQDQ	X3, X2, X2
	VINSERTI128	$1, X2, Y0, Y0
	VMOVDQU	Y0, 15*32(SP)
	VMOVQ	128(DI), X0
	VMOVQ	128(SI), X1
	VPUNPCKLQDQ	X1, X0, X0
	VMOVQ	128(DX), X2
	VMOVQ	128(CX), X3
	VPUNPCKLQDQ	X3, X2, X2
	VINSERTI128	$1, X2, Y0, Y0
	VMOVDQU	Y0, 16*32(SP)
	VMOVQ	136(DI), X0
	VMOVQ	136(SI), X1
	VPUNPCKLQDQ	X1, X0, X0
	VMOVQ	136(DX), X2
	VMOVQ	136(CX), X3
	VPUNPCKLQDQ	X3, X2, X2
	VINSERTI128	$1, X2, Y0, Y0
	VMOVDQU	Y0, 17*32(SP)
	VMOVQ	144(DI), X0
	VMOVQ	144(SI), X1
	VPUNPCKLQDQ	X1, X0, X0
	VMOVQ	144(DX), X2
	VMOVQ	144(CX), X3
	VPUNPCKLQDQ	X3, X2, X2
	VINSERTI128	$1, X2, Y0, Y0
	VMOVDQU	Y0, 18*32(SP)
	VMOVQ	152(DI), X0
	VMOVQ	152(SI), X1
	VPUNPCKLQDQ	X1, X0, X0
	VMOVQ	152(DX), X2
	VMOVQ	152(CX), X3
	VPUNPCKLQDQ	X3, X2, X2
	VINSERTI128	$1, X2, Y0, Y0
	VMOVDQU	Y0, 19*32(SP)
	VMOVQ	160(DI), X0
	VMOVQ	160(SI), X1
	VPUNPCKLQDQ	X1, X0, X0
	VMOVQ	160(DX), X2
	VMOVQ	160(CX), X3
	VPUNPCKLQDQ	X3, X2, X2
	VINSERTI128	$1, X2, Y0, Y0
	VMOVDQU	Y0, 20*32(SP)
	VMOVQ	168(DI), X0
	VMOVQ	168(SI), X1
	VPUNPCKLQDQ	X1, X0, X0
	VMOVQ	168(DX), X2
	VMOVQ	168(CX), X3
	VPUNPCKLQDQ	X3, X2, X2
	VINSERTI128	$1, X2, Y0, Y0
	VMOVDQU	Y0, 21*32(SP)
	VMOVQ	176(DI), X0
	VMOVQ	176(SI), X1
	VPUNPCKLQDQ	X1, X0, X0
	VMOVQ	176(DX), X2
	VMOVQ	176(CX), X3
	VPUNPCKLQDQ	X3, X2, X2
	VINSERTI128	$1, X2, Y0, Y0
	VMOVDQU	Y0, 22*32(SP)
	VMOVQ	184(DI), X0
	VMOVQ	184(SI), X1
	VPUNPCKLQDQ	X1, X0, X0
	VMOVQ	184(DX), X2
	VMOVQ	184(CX), X3
	VPUNPCKLQDQ	X3, X2, X2
	VINSERTI128	$1, X2, Y0, Y0
	VMOVDQU	Y0, 23*32(SP)
	VMOVQ	192(DI), X0
	VMOVQ	192(SI), X1
	VPUNPCKLQDQ	X1, X0, X0
	VMOVQ	192(DX), X2
	VMOVQ	192(CX), X3
	VPUNPCKLQDQ	X3, X2, X2
	VINSERTI128	$1, X2, Y0, Y0
	VMOVDQU	Y0, 24*32(SP)
	LEAQ	0(SP), R8
	LEAQ	800(SP), R9
	LEAQ	round_consts_4x<>+384(SB), R11
	MOVQ	$12, R10
round_loop:
	VMOVDQU	0*32(R8), Y0
	VMOVDQU	5*32(R8), Y14
	VPXOR	Y14, Y0, Y0
	VMOVDQU	10*32(R8), Y14
	VPXOR	Y14, Y0, Y0
	VMOVDQU	15*32(R8), Y14
	VPXOR	Y14, Y0, Y0
	VMOVDQU	20*32(R8), Y14
	VPXOR	Y14, Y0, Y0
	VMOVDQU	1*32(R8), Y1
	VMOVDQU	6*32(R8), Y14
	VPXOR	Y14, Y1, Y1
	VMOVDQU	11*32(R8), Y14
	VPXOR	Y14, Y1, Y1
	VMOVDQU	16*32(R8), Y14
	VPXOR	Y14, Y1, Y1
	VMOVDQU	21*32(R8), Y14
	VPXOR	Y14, Y1, Y1
	VMOVDQU	2*32(R8), Y2
	VMOVDQU	7*32(R8), Y14
	VPXOR	Y14, Y2, Y2
	VMOVDQU	12*32(R8), Y14
	VPXOR	Y14, Y2, Y2
	VMOVDQU	17*32(R8), Y14
	VPXOR	Y14, Y2, Y2
	VMOVDQU	22*32(R8), Y14
	VPXOR	Y14, Y2, Y2
	VMOVDQU	3*32(R8), Y3
	VMOVDQU	8*32(R8), Y14
	VPXOR	Y14, Y3, Y3
	VMOVDQU	13*32(R8), Y14
	VPXOR	Y14, Y3, Y3
	VMOVDQU	18*32(R8), Y14
	VPXOR	Y14, Y3, Y3
	VMOVDQU	23*32(R8), Y14
	VPXOR	Y14, Y3, Y3
	VMOVDQU	4*32(R8), Y4
	VMOVDQU	9*32(R8), Y14
	VPXOR	Y14, Y4, Y4
	VMOVDQU	14*32(R8), Y14
	VPXOR	Y14, Y4, Y4
	VMOVDQU	19*32(R8), Y14
	VPXOR	Y14, Y4, Y4
	VMOVDQU	24*32(R8), Y14
	VPXOR	Y14, Y4, Y4
	VMOVDQU	Y1, Y5
	ROT64_AVX512(Y5, 1)
	VPXOR	Y4, Y5, Y5
	VMOVDQU	Y2, Y6
	ROT64_AVX512(Y6, 1)
	VPXOR	Y0, Y6, Y6
	VMOVDQU	Y3, Y7
	ROT64_AVX512(Y7, 1)
	VPXOR	Y1, Y7, Y7
	VMOVDQU	Y4, Y8
	ROT64_AVX512(Y8, 1)
	VPXOR	Y2, Y8, Y8
	VMOVDQU	Y0, Y9
	ROT64_AVX512(Y9, 1)
	VPXOR	Y3, Y9, Y9
	VMOVDQU	0*32(R8), Y0
	VPXOR	Y5, Y0, Y0
	VMOVDQU	6*32(R8), Y1
	VPXOR	Y6, Y1, Y1
	ROT64_AVX512(Y1, 44)
	VMOVDQU	12*32(R8), Y2
	VPXOR	Y7, Y2, Y2
	ROT64_AVX512(Y2, 43)
	VMOVDQU	18*32(R8), Y3
	VPXOR	Y8, Y3, Y3
	ROT64_AVX512(Y3, 21)
	VMOVDQU	24*32(R8), Y4
	VPXOR	Y9, Y4, Y4
	ROT64_AVX512(Y4, 14)
	VMOVDQU	(R11), Y15
	CHI_IOTA_AVX512(0)
	VMOVDQU	3*32(R8), Y0
	VPXOR	Y8, Y0, Y0
	ROT64_AVX512(Y0, 28)
	VMOVDQU	9*32(R8), Y1
	VPXOR	Y9, Y1, Y1
	ROT64_AVX512(Y1, 20)
	VMOVDQU	10*32(R8), Y2
	VPXOR	Y5, Y2, Y2
	ROT64_AVX512(Y2, 3)
	VMOVDQU	16*32(R8), Y3
	VPXOR	Y6, Y3, Y3
	ROT64_AVX512(Y3, 45)
	VMOVDQU	22*32(R8), Y4
	VPXOR	Y7, Y4, Y4
	ROT64_AVX512(Y4, 61)
	CHI_AVX512(5)
	VMOVDQU	1*32(R8), Y0
	VPXOR	Y6, Y0, Y0
	ROT64_AVX512(Y0, 1)
	VMOVDQU	7*32(R8), Y1
	VPXOR	Y7, Y1, Y1
	ROT64_AVX512(Y1, 6)
	VMOVDQU	13*32(R8), Y2
	VPXOR	Y8, Y2, Y2
	ROT64_AVX512(Y2, 25)
	VMOVDQU	19*32(R8), Y3
	VPXOR	Y9, Y3, Y3
	ROT64_AVX512(Y3, 8)
	VMOVDQU	20*32(R8), Y4
	VPXOR	Y5, Y4, Y4
	ROT64_AVX512(Y4, 18)
	CHI_AVX512(10)
	VMOVDQU	4*32(R8), Y0
	VPXOR	Y9, Y0, Y0
	ROT64_AVX512(Y0, 27)
	VMOVDQU	5*32(R8), Y1
	VPXOR	Y5, Y1, Y1
	ROT64_AVX512(Y1, 36)
	VMOVDQU	11*32(R8), Y2
	VPXOR	Y6, Y2, Y2
	ROT64_AVX512(Y2, 10)
	VMOVDQU	17*32(R8), Y3
	VPXOR	Y7, Y3, Y3
	ROT64_AVX512(Y3, 15)
	VMOVDQU	23*32(R8), Y4
	VPXOR	Y8, Y4, Y4
	ROT64_AVX512(Y4, 56)
	CHI_AVX512(15)
	VMOVDQU	2*32(R8), Y0
	VPXOR	Y7, Y0, Y0
	ROT64_AVX512(Y0, 62)
	VMOVDQU	8*32(R8), Y1
	VPXOR	Y8, Y1, Y1
	ROT64_AVX512(Y1, 55)
	VMOVDQU	14*32(R8), Y2
	VPXOR	Y9, Y2, Y2
	ROT64_AVX512(Y2, 39)
	VMOVDQU	15*32(R8), Y3
	VPXOR	Y5, Y3, Y3
	ROT64_AVX512(Y3, 41)
	VMOVDQU	21*32(R8), Y4
	VPXOR	Y6, Y4, Y4
	ROT64_AVX512(Y4, 2)
	CHI_AVX512(20)
	XCHGQ	R8, R9
	ADDQ	$32, R11
	SUBQ	$1, R10
	JNZ	round_loop
	VMOVDQU	0*32(R8), Y0
	VMOVQ	X0, 0(DI)
	VPEXTRQ	$1, X0, 0(SI)
	VEXTRACTI128	$1, Y0, X2
	VMOVQ	X2, 0(DX)
	VPEXTRQ	$1, X2, 0(CX)
	VMOVDQU	1*32(R8), Y0
	VMOVQ	X0, 8(DI)
	VPEXTRQ	$1, X0, 8(SI)
	VEXTRACTI128	$1, Y0, X2
	VMOVQ	X2, 8(DX)
	VPEXTRQ	$1, X2, 8(CX)
	VMOVDQU	2*32(R8), Y0
	VMOVQ	X0, 16(DI)
	VPEXTRQ	$1, X0, 16(SI)
	VEXTRACTI128	$1, Y0, X2
	VMOVQ	X2, 16(DX)
	VPEXTRQ	$1, X2, 16(CX)
	VMOVDQU	3*32(R8), Y0
	VMOVQ	X0, 24(DI)
	VPEXTRQ	$1, X0, 24(SI)
	VEXTRACTI128	$1, Y0, X2
	VMOVQ	X2, 24(DX)
	VPEXTRQ	$1, X2, 24(CX)
	VMOVDQU	4*32(R8), Y0
	VMOVQ	X0, 32(DI)
	VPEXTRQ	$1, X0, 32(SI)
	VEXTRACTI128	$1, Y0, X2
	VMOVQ	X2, 32(DX)
	VPEXTRQ	$1, X2, 32(CX)
	VMOVDQU	5*32(R8), Y0
	VMOVQ	X0, 40(DI)
	VPEXTRQ	$1, X0, 40(SI)
	VEXTRACTI128	$1, Y0, X2
	VMOVQ	X2, 40(DX)
	VPEXTRQ	$1, X2, 40(CX)
	VMOVDQU	6*32(R8), Y0
	VMOVQ	X0, 48(DI)
	VPEXTRQ	$1, X0, 48(SI)
	VEXTRACTI128	$1, Y0, X2
	VMOVQ	X2, 48(DX)
	VPEXTRQ	$1, X2, 48(CX)
	VMOVDQU	7*32(R8), Y0
	VMOVQ	X0, 56(DI)
	VPEXTRQ	$1, X0, 56(SI)
	VEXTRACTI128	$1, Y0, X2
	VMOVQ	X2, 56(DX)
	VPEXTRQ	$1, X2, 56(CX)
	VMOVDQU	8*32(R8), Y0
	VMOVQ	X0, 64(DI)
	VPEXTRQ	$1, X0, 64(SI)
	VEXTRACTI128	$1, Y0, X2
	VMOVQ	X2, 64(DX)
	VPEXTRQ	$1, X2, 64(CX)
	VMOVDQU	9*32(R8), Y0
	VMOVQ	X0, 72(DI)
	VPEXTRQ	$1, X0, 72(SI)
	VEXTRACTI128	$1, Y0, X2
	VMOVQ	X2, 72(DX)
	VPEXTRQ	$1, X2, 72(CX)
	VMOVDQU	10*32(R8), Y0
	VMOVQ	X0, 80(DI)
	VPEXTRQ	$1, X0, 80(SI)
	VEXTRACTI128	$1, Y0, X2
	VMOVQ	X2, 80(DX)
	VPEXTRQ	$1, X2, 80(CX)
	VMOVDQU	11*32(R8), Y0
	VMOVQ	X0, 88(DI)
	VPEXTRQ	$1, X0, 88(SI)
	VEXTRACTI128	$1, Y0, X2
	VMOVQ	X2, 88(DX)
	VPEXTRQ	$1, X2, 88(CX)
	VMOVDQU	12*32(R8), Y0
	VMOVQ	X0, 96(DI)
	VPEXTRQ	$1, X0, 96(SI)
	VEXTRACTI128	$1, Y0, X2
	VMOVQ	X2, 96(DX)
	VPEXTRQ	$1, X2, 96(CX)
	VMOVDQU	13*32(R8), Y0
	VMOVQ	X0, 104(DI)
	VPEXTRQ	$1, X0, 104(SI)
	VEXTRACTI128	$1, Y0, X2
	VMOVQ	X2, 104(DX)
	VPEXTRQ	$1, X2, 104(CX)
	VMOVDQU	14*32(R8), Y0
	VMOVQ	X0, 112(DI)
	VPEXTRQ	$1, X0, 112(SI)
	VEXTRACTI128	$1, Y0, X2
	VMOVQ	X2, 112(DX)
	VPEXTRQ	$1, X2, 112(CX)
	VMOVDQU	15*32(R8), Y0
	VMOVQ	X0, 120(DI)
	VPEXTRQ	$1, X0, 120(SI)
	VEXTRACTI128	$1, Y0, X2
	VMOVQ	X2, 120(DX)
	VPEXTRQ	$1, X2, 120(CX)
	VMOVDQU	16*32(R8), Y0
	VMOVQ	X0, 128(DI)
	VPEXTRQ	$1, X0, 128(SI)
	VEXTRACTI128	$1, Y0, X2
	VMOVQ	X2, 128(DX)
	VPEXTRQ	$1, X2, 128(CX)
	VMOVDQU	17*32(R8), Y0
	VMOVQ	X0, 136(DI)
	VPEXTRQ	$1, X0, 136(SI)
	VEXTRACTI128	$1, Y0, X2
	VMOVQ	X2, 136(DX)
	VPEXTRQ	$1, X2, 136(CX)
	VMOVDQU	18*32(R8), Y0
	VMOVQ	X0, 144(DI)
	VPEXTRQ	$1, X0, 144(SI)
	VEXTRACTI128	$1, Y0, X2
	VMOVQ	X2, 144(DX)
	VPEXTRQ	$1, X2, 144(CX)
	VMOVDQU	19*32(R8), Y0
	VMOVQ	X0, 152(DI)
	VPEXTRQ	$1, X0, 152(SI)
	VEXTRACTI128	$1, Y0, X2
	VMOVQ	X2, 152(DX)
	VPEXTRQ	$1, X2, 152(CX)
	VMOVDQU	20*32(R8), Y0
	VMOVQ	X0, 160(DI)
	VPEXTRQ	$1, X0, 160(SI)
	VEXTRACTI128	$1, Y0, X2
	VMOVQ	X2, 160(DX)
	VPEXTRQ	$1, X2, 160(CX)
	VMOVDQU	21*32(R8), Y0
	VMOVQ	X0, 168(DI)
	VPEXTRQ	$1, X0, 168(SI)
	VEXTRACTI128	$1, Y0, X2
	VMOVQ	X2, 168(DX)
	VPEXTRQ	$1, X2, 168(CX)
	VMOVDQU	22*32(R8), Y0
	VMOVQ	X0, 176(DI)
	VPEXTRQ	$1, X0, 176(SI)
	VEXTRACTI128	$1, Y0, X2
	VMOVQ	X2, 176(DX)
	VPEXTRQ	$1, X2, 176(CX)
	VMOVDQU	23*32(R8), Y0
	VMOVQ	X0, 184(DI)
	VPEXTRQ	$1, X0, 184(SI)
	VEXTRACTI128	$1, Y0, X2
	VMOVQ	X2, 184(DX)
	VPEXTRQ	$1, X2, 184(CX)
	VMOVDQU	24*32(R8), Y0
	VMOVQ	X0, 192(DI)
	VPEXTRQ	$1, X0, 192(SI)
	VEXTRACTI128	$1, Y0, X2
	VMOVQ	X2, 192(DX)
	VPEXTRQ	$1, X2, 192(CX)
	VZEROUPPER
	RET

#define ROT64_AVX2(reg, amount) \
	VMOVDQU	reg, Y13; \
	VPSLLQ	$amount, reg, reg; \
	VPSRLQ	$(64-amount), Y13, Y13; \
	VPOR	Y13, reg, reg

#define CHI_AVX2(base) \
	VMOVDQU	Y0, Y10; \
	VMOVDQU	Y1, Y11; \
	VMOVDQU	Y1, Y12; \
	VPANDN	Y2, Y12, Y12; \
	VPXOR	Y0, Y12, Y12; \
	VMOVDQU	Y12, (base+0)*32(R9); \
	VMOVDQU	Y2, Y12; \
	VPANDN	Y3, Y12, Y12; \
	VPXOR	Y1, Y12, Y12; \
	VMOVDQU	Y12, (base+1)*32(R9); \
	VMOVDQU	Y3, Y12; \
	VPANDN	Y4, Y12, Y12; \
	VPXOR	Y2, Y12, Y12; \
	VMOVDQU	Y12, (base+2)*32(R9); \
	VMOVDQU	Y4, Y12; \
	VPANDN	Y10, Y12, Y12; \
	VPXOR	Y3, Y12, Y12; \
	VMOVDQU	Y12, (base+3)*32(R9); \
	VPANDN	Y11, Y10, Y10; \
	VPXOR	Y4, Y10, Y10; \
	VMOVDQU	Y10, (base+4)*32(R9)

#define CHI_IOTA_AVX2(base) \
	VMOVDQU	Y0, Y10; \
	VMOVDQU	Y1, Y11; \
	VMOVDQU	Y1, Y12; \
	VPANDN	Y2, Y12, Y12; \
	VPXOR	Y0, Y12, Y12; \
	VPXOR	Y15, Y12, Y12; \
	VMOVDQU	Y12, (base+0)*32(R9); \
	VMOVDQU	Y2, Y12; \
	VPANDN	Y3, Y12, Y12; \
	VPXOR	Y1, Y12, Y12; \
	VMOVDQU	Y12, (base+1)*32(R9); \
	VMOVDQU	Y3, Y12; \
	VPANDN	Y4, Y12, Y12; \
	VPXOR	Y2, Y12, Y12; \
	VMOVDQU	Y12, (base+2)*32(R9); \
	VMOVDQU	Y4, Y12; \
	VPANDN	Y10, Y12, Y12; \
	VPXOR	Y3, Y12, Y12; \
	VMOVDQU	Y12, (base+3)*32(R9); \
	VPANDN	Y11, Y10, Y10; \
	VPXOR	Y4, Y10, Y10; \
	VMOVDQU	Y10, (base+4)*32(R9)

TEXT ·p1600x4AVX2(SB), $1600-32
	MOVQ	a+0(FP), DI
	MOVQ	b+8(FP), SI
	MOVQ	c+16(FP), DX
	MOVQ	d+24(FP), CX


	VMOVQ	0(DI), X0
	VMOVQ	0(SI), X1
	VPUNPCKLQDQ	X1, X0, X0
	VMOVQ	0(DX), X2
	VMOVQ	0(CX), X3
	VPUNPCKLQDQ	X3, X2, X2
	VINSERTI128	$1, X2, Y0, Y0
	VMOVDQU	Y0, 0*32(SP)

	VMOVQ	8(DI), X0
	VMOVQ	8(SI), X1
	VPUNPCKLQDQ	X1, X0, X0
	VMOVQ	8(DX), X2
	VMOVQ	8(CX), X3
	VPUNPCKLQDQ	X3, X2, X2
	VINSERTI128	$1, X2, Y0, Y0
	VMOVDQU	Y0, 1*32(SP)

	VMOVQ	16(DI), X0
	VMOVQ	16(SI), X1
	VPUNPCKLQDQ	X1, X0, X0
	VMOVQ	16(DX), X2
	VMOVQ	16(CX), X3
	VPUNPCKLQDQ	X3, X2, X2
	VINSERTI128	$1, X2, Y0, Y0
	VMOVDQU	Y0, 2*32(SP)

	VMOVQ	24(DI), X0
	VMOVQ	24(SI), X1
	VPUNPCKLQDQ	X1, X0, X0
	VMOVQ	24(DX), X2
	VMOVQ	24(CX), X3
	VPUNPCKLQDQ	X3, X2, X2
	VINSERTI128	$1, X2, Y0, Y0
	VMOVDQU	Y0, 3*32(SP)

	VMOVQ	32(DI), X0
	VMOVQ	32(SI), X1
	VPUNPCKLQDQ	X1, X0, X0
	VMOVQ	32(DX), X2
	VMOVQ	32(CX), X3
	VPUNPCKLQDQ	X3, X2, X2
	VINSERTI128	$1, X2, Y0, Y0
	VMOVDQU	Y0, 4*32(SP)

	VMOVQ	40(DI), X0
	VMOVQ	40(SI), X1
	VPUNPCKLQDQ	X1, X0, X0
	VMOVQ	40(DX), X2
	VMOVQ	40(CX), X3
	VPUNPCKLQDQ	X3, X2, X2
	VINSERTI128	$1, X2, Y0, Y0
	VMOVDQU	Y0, 5*32(SP)

	VMOVQ	48(DI), X0
	VMOVQ	48(SI), X1
	VPUNPCKLQDQ	X1, X0, X0
	VMOVQ	48(DX), X2
	VMOVQ	48(CX), X3
	VPUNPCKLQDQ	X3, X2, X2
	VINSERTI128	$1, X2, Y0, Y0
	VMOVDQU	Y0, 6*32(SP)

	VMOVQ	56(DI), X0
	VMOVQ	56(SI), X1
	VPUNPCKLQDQ	X1, X0, X0
	VMOVQ	56(DX), X2
	VMOVQ	56(CX), X3
	VPUNPCKLQDQ	X3, X2, X2
	VINSERTI128	$1, X2, Y0, Y0
	VMOVDQU	Y0, 7*32(SP)

	VMOVQ	64(DI), X0
	VMOVQ	64(SI), X1
	VPUNPCKLQDQ	X1, X0, X0
	VMOVQ	64(DX), X2
	VMOVQ	64(CX), X3
	VPUNPCKLQDQ	X3, X2, X2
	VINSERTI128	$1, X2, Y0, Y0
	VMOVDQU	Y0, 8*32(SP)

	VMOVQ	72(DI), X0
	VMOVQ	72(SI), X1
	VPUNPCKLQDQ	X1, X0, X0
	VMOVQ	72(DX), X2
	VMOVQ	72(CX), X3
	VPUNPCKLQDQ	X3, X2, X2
	VINSERTI128	$1, X2, Y0, Y0
	VMOVDQU	Y0, 9*32(SP)

	VMOVQ	80(DI), X0
	VMOVQ	80(SI), X1
	VPUNPCKLQDQ	X1, X0, X0
	VMOVQ	80(DX), X2
	VMOVQ	80(CX), X3
	VPUNPCKLQDQ	X3, X2, X2
	VINSERTI128	$1, X2, Y0, Y0
	VMOVDQU	Y0, 10*32(SP)

	VMOVQ	88(DI), X0
	VMOVQ	88(SI), X1
	VPUNPCKLQDQ	X1, X0, X0
	VMOVQ	88(DX), X2
	VMOVQ	88(CX), X3
	VPUNPCKLQDQ	X3, X2, X2
	VINSERTI128	$1, X2, Y0, Y0
	VMOVDQU	Y0, 11*32(SP)

	VMOVQ	96(DI), X0
	VMOVQ	96(SI), X1
	VPUNPCKLQDQ	X1, X0, X0
	VMOVQ	96(DX), X2
	VMOVQ	96(CX), X3
	VPUNPCKLQDQ	X3, X2, X2
	VINSERTI128	$1, X2, Y0, Y0
	VMOVDQU	Y0, 12*32(SP)

	VMOVQ	104(DI), X0
	VMOVQ	104(SI), X1
	VPUNPCKLQDQ	X1, X0, X0
	VMOVQ	104(DX), X2
	VMOVQ	104(CX), X3
	VPUNPCKLQDQ	X3, X2, X2
	VINSERTI128	$1, X2, Y0, Y0
	VMOVDQU	Y0, 13*32(SP)

	VMOVQ	112(DI), X0
	VMOVQ	112(SI), X1
	VPUNPCKLQDQ	X1, X0, X0
	VMOVQ	112(DX), X2
	VMOVQ	112(CX), X3
	VPUNPCKLQDQ	X3, X2, X2
	VINSERTI128	$1, X2, Y0, Y0
	VMOVDQU	Y0, 14*32(SP)

	VMOVQ	120(DI), X0
	VMOVQ	120(SI), X1
	VPUNPCKLQDQ	X1, X0, X0
	VMOVQ	120(DX), X2
	VMOVQ	120(CX), X3
	VPUNPCKLQDQ	X3, X2, X2
	VINSERTI128	$1, X2, Y0, Y0
	VMOVDQU	Y0, 15*32(SP)

	VMOVQ	128(DI), X0
	VMOVQ	128(SI), X1
	VPUNPCKLQDQ	X1, X0, X0
	VMOVQ	128(DX), X2
	VMOVQ	128(CX), X3
	VPUNPCKLQDQ	X3, X2, X2
	VINSERTI128	$1, X2, Y0, Y0
	VMOVDQU	Y0, 16*32(SP)

	VMOVQ	136(DI), X0
	VMOVQ	136(SI), X1
	VPUNPCKLQDQ	X1, X0, X0
	VMOVQ	136(DX), X2
	VMOVQ	136(CX), X3
	VPUNPCKLQDQ	X3, X2, X2
	VINSERTI128	$1, X2, Y0, Y0
	VMOVDQU	Y0, 17*32(SP)

	VMOVQ	144(DI), X0
	VMOVQ	144(SI), X1
	VPUNPCKLQDQ	X1, X0, X0
	VMOVQ	144(DX), X2
	VMOVQ	144(CX), X3
	VPUNPCKLQDQ	X3, X2, X2
	VINSERTI128	$1, X2, Y0, Y0
	VMOVDQU	Y0, 18*32(SP)

	VMOVQ	152(DI), X0
	VMOVQ	152(SI), X1
	VPUNPCKLQDQ	X1, X0, X0
	VMOVQ	152(DX), X2
	VMOVQ	152(CX), X3
	VPUNPCKLQDQ	X3, X2, X2
	VINSERTI128	$1, X2, Y0, Y0
	VMOVDQU	Y0, 19*32(SP)

	VMOVQ	160(DI), X0
	VMOVQ	160(SI), X1
	VPUNPCKLQDQ	X1, X0, X0
	VMOVQ	160(DX), X2
	VMOVQ	160(CX), X3
	VPUNPCKLQDQ	X3, X2, X2
	VINSERTI128	$1, X2, Y0, Y0
	VMOVDQU	Y0, 20*32(SP)

	VMOVQ	168(DI), X0
	VMOVQ	168(SI), X1
	VPUNPCKLQDQ	X1, X0, X0
	VMOVQ	168(DX), X2
	VMOVQ	168(CX), X3
	VPUNPCKLQDQ	X3, X2, X2
	VINSERTI128	$1, X2, Y0, Y0
	VMOVDQU	Y0, 21*32(SP)

	VMOVQ	176(DI), X0
	VMOVQ	176(SI), X1
	VPUNPCKLQDQ	X1, X0, X0
	VMOVQ	176(DX), X2
	VMOVQ	176(CX), X3
	VPUNPCKLQDQ	X3, X2, X2
	VINSERTI128	$1, X2, Y0, Y0
	VMOVDQU	Y0, 22*32(SP)

	VMOVQ	184(DI), X0
	VMOVQ	184(SI), X1
	VPUNPCKLQDQ	X1, X0, X0
	VMOVQ	184(DX), X2
	VMOVQ	184(CX), X3
	VPUNPCKLQDQ	X3, X2, X2
	VINSERTI128	$1, X2, Y0, Y0
	VMOVDQU	Y0, 23*32(SP)

	VMOVQ	192(DI), X0
	VMOVQ	192(SI), X1
	VPUNPCKLQDQ	X1, X0, X0
	VMOVQ	192(DX), X2
	VMOVQ	192(CX), X3
	VPUNPCKLQDQ	X3, X2, X2
	VINSERTI128	$1, X2, Y0, Y0
	VMOVDQU	Y0, 24*32(SP)


	// Set up loop
	LEAQ	0(SP), R8
	LEAQ	800(SP), R9
	LEAQ	round_consts_4x<>+384(SB), R11     // RC start (round 12)
	MOVQ	$12, R10

	PCALIGN	$16
round_loop:
	VMOVDQU	0*32(R8), Y0
	VMOVDQU	5*32(R8), Y14
	VPXOR	Y14, Y0, Y0
	VMOVDQU	10*32(R8), Y14
	VPXOR	Y14, Y0, Y0
	VMOVDQU	15*32(R8), Y14
	VPXOR	Y14, Y0, Y0
	VMOVDQU	20*32(R8), Y14
	VPXOR	Y14, Y0, Y0

	VMOVDQU	1*32(R8), Y1
	VMOVDQU	6*32(R8), Y14
	VPXOR	Y14, Y1, Y1
	VMOVDQU	11*32(R8), Y14
	VPXOR	Y14, Y1, Y1
	VMOVDQU	16*32(R8), Y14
	VPXOR	Y14, Y1, Y1
	VMOVDQU	21*32(R8), Y14
	VPXOR	Y14, Y1, Y1

	VMOVDQU	2*32(R8), Y2
	VMOVDQU	7*32(R8), Y14
	VPXOR	Y14, Y2, Y2
	VMOVDQU	12*32(R8), Y14
	VPXOR	Y14, Y2, Y2
	VMOVDQU	17*32(R8), Y14
	VPXOR	Y14, Y2, Y2
	VMOVDQU	22*32(R8), Y14
	VPXOR	Y14, Y2, Y2

	VMOVDQU	3*32(R8), Y3
	VMOVDQU	8*32(R8), Y14
	VPXOR	Y14, Y3, Y3
	VMOVDQU	13*32(R8), Y14
	VPXOR	Y14, Y3, Y3
	VMOVDQU	18*32(R8), Y14
	VPXOR	Y14, Y3, Y3
	VMOVDQU	23*32(R8), Y14
	VPXOR	Y14, Y3, Y3

	VMOVDQU	4*32(R8), Y4
	VMOVDQU	9*32(R8), Y14
	VPXOR	Y14, Y4, Y4
	VMOVDQU	14*32(R8), Y14
	VPXOR	Y14, Y4, Y4
	VMOVDQU	19*32(R8), Y14
	VPXOR	Y14, Y4, Y4
	VMOVDQU	24*32(R8), Y14
	VPXOR	Y14, Y4, Y4

	VMOVDQU	Y1, Y5
	ROT64_AVX2(Y5, 1)
	VPXOR	Y4, Y5, Y5

	VMOVDQU	Y2, Y6
	ROT64_AVX2(Y6, 1)
	VPXOR	Y0, Y6, Y6

	VMOVDQU	Y3, Y7
	ROT64_AVX2(Y7, 1)
	VPXOR	Y1, Y7, Y7

	VMOVDQU	Y4, Y8
	ROT64_AVX2(Y8, 1)
	VPXOR	Y2, Y8, Y8

	VMOVDQU	Y0, Y9
	ROT64_AVX2(Y9, 1)
	VPXOR	Y3, Y9, Y9

	VMOVDQU	0*32(R8), Y0
	VPXOR	Y5, Y0, Y0

	VMOVDQU	6*32(R8), Y1
	VPXOR	Y6, Y1, Y1
	ROT64_AVX2(Y1, 44)

	VMOVDQU	12*32(R8), Y2
	VPXOR	Y7, Y2, Y2
	ROT64_AVX2(Y2, 43)

	VMOVDQU	18*32(R8), Y3
	VPXOR	Y8, Y3, Y3
	ROT64_AVX2(Y3, 21)

	VMOVDQU	24*32(R8), Y4
	VPXOR	Y9, Y4, Y4
	ROT64_AVX2(Y4, 14)

	VMOVDQU	(R11), Y15
	CHI_IOTA_AVX2(0)

	VMOVDQU	3*32(R8), Y0
	VPXOR	Y8, Y0, Y0
	ROT64_AVX2(Y0, 28)

	VMOVDQU	9*32(R8), Y1
	VPXOR	Y9, Y1, Y1
	ROT64_AVX2(Y1, 20)

	VMOVDQU	10*32(R8), Y2
	VPXOR	Y5, Y2, Y2
	ROT64_AVX2(Y2, 3)

	VMOVDQU	16*32(R8), Y3
	VPXOR	Y6, Y3, Y3
	ROT64_AVX2(Y3, 45)

	VMOVDQU	22*32(R8), Y4
	VPXOR	Y7, Y4, Y4
	ROT64_AVX2(Y4, 61)

	CHI_AVX2(5)

	VMOVDQU	1*32(R8), Y0
	VPXOR	Y6, Y0, Y0
	ROT64_AVX2(Y0, 1)

	VMOVDQU	7*32(R8), Y1
	VPXOR	Y7, Y1, Y1
	ROT64_AVX2(Y1, 6)

	VMOVDQU	13*32(R8), Y2
	VPXOR	Y8, Y2, Y2
	ROT64_AVX2(Y2, 25)

	VMOVDQU	19*32(R8), Y3
	VPXOR	Y9, Y3, Y3
	ROT64_AVX2(Y3, 8)

	VMOVDQU	20*32(R8), Y4
	VPXOR	Y5, Y4, Y4
	ROT64_AVX2(Y4, 18)

	CHI_AVX2(10)

	VMOVDQU	4*32(R8), Y0
	VPXOR	Y9, Y0, Y0
	ROT64_AVX2(Y0, 27)

	VMOVDQU	5*32(R8), Y1
	VPXOR	Y5, Y1, Y1
	ROT64_AVX2(Y1, 36)

	VMOVDQU	11*32(R8), Y2
	VPXOR	Y6, Y2, Y2
	ROT64_AVX2(Y2, 10)

	VMOVDQU	17*32(R8), Y3
	VPXOR	Y7, Y3, Y3
	ROT64_AVX2(Y3, 15)

	VMOVDQU	23*32(R8), Y4
	VPXOR	Y8, Y4, Y4
	ROT64_AVX2(Y4, 56)

	CHI_AVX2(15)

	VMOVDQU	2*32(R8), Y0
	VPXOR	Y7, Y0, Y0
	ROT64_AVX2(Y0, 62)

	VMOVDQU	8*32(R8), Y1
	VPXOR	Y8, Y1, Y1
	ROT64_AVX2(Y1, 55)

	VMOVDQU	14*32(R8), Y2
	VPXOR	Y9, Y2, Y2
	ROT64_AVX2(Y2, 39)

	VMOVDQU	15*32(R8), Y3
	VPXOR	Y5, Y3, Y3
	ROT64_AVX2(Y3, 41)

	VMOVDQU	21*32(R8), Y4
	VPXOR	Y6, Y4, Y4
	ROT64_AVX2(Y4, 2)

	CHI_AVX2(20)

	XCHGQ	R8, R9
	ADDQ	$32, R11
	SUBQ	$1, R10
	JNZ	round_loop


	VMOVDQU	0*32(R8), Y0
	VMOVQ	X0, 0(DI)
	VPEXTRQ	$1, X0, 0(SI)
	VEXTRACTI128	$1, Y0, X2
	VMOVQ	X2, 0(DX)
	VPEXTRQ	$1, X2, 0(CX)

	VMOVDQU	1*32(R8), Y0
	VMOVQ	X0, 8(DI)
	VPEXTRQ	$1, X0, 8(SI)
	VEXTRACTI128	$1, Y0, X2
	VMOVQ	X2, 8(DX)
	VPEXTRQ	$1, X2, 8(CX)

	VMOVDQU	2*32(R8), Y0
	VMOVQ	X0, 16(DI)
	VPEXTRQ	$1, X0, 16(SI)
	VEXTRACTI128	$1, Y0, X2
	VMOVQ	X2, 16(DX)
	VPEXTRQ	$1, X2, 16(CX)

	VMOVDQU	3*32(R8), Y0
	VMOVQ	X0, 24(DI)
	VPEXTRQ	$1, X0, 24(SI)
	VEXTRACTI128	$1, Y0, X2
	VMOVQ	X2, 24(DX)
	VPEXTRQ	$1, X2, 24(CX)

	VMOVDQU	4*32(R8), Y0
	VMOVQ	X0, 32(DI)
	VPEXTRQ	$1, X0, 32(SI)
	VEXTRACTI128	$1, Y0, X2
	VMOVQ	X2, 32(DX)
	VPEXTRQ	$1, X2, 32(CX)

	VMOVDQU	5*32(R8), Y0
	VMOVQ	X0, 40(DI)
	VPEXTRQ	$1, X0, 40(SI)
	VEXTRACTI128	$1, Y0, X2
	VMOVQ	X2, 40(DX)
	VPEXTRQ	$1, X2, 40(CX)

	VMOVDQU	6*32(R8), Y0
	VMOVQ	X0, 48(DI)
	VPEXTRQ	$1, X0, 48(SI)
	VEXTRACTI128	$1, Y0, X2
	VMOVQ	X2, 48(DX)
	VPEXTRQ	$1, X2, 48(CX)

	VMOVDQU	7*32(R8), Y0
	VMOVQ	X0, 56(DI)
	VPEXTRQ	$1, X0, 56(SI)
	VEXTRACTI128	$1, Y0, X2
	VMOVQ	X2, 56(DX)
	VPEXTRQ	$1, X2, 56(CX)

	VMOVDQU	8*32(R8), Y0
	VMOVQ	X0, 64(DI)
	VPEXTRQ	$1, X0, 64(SI)
	VEXTRACTI128	$1, Y0, X2
	VMOVQ	X2, 64(DX)
	VPEXTRQ	$1, X2, 64(CX)

	VMOVDQU	9*32(R8), Y0
	VMOVQ	X0, 72(DI)
	VPEXTRQ	$1, X0, 72(SI)
	VEXTRACTI128	$1, Y0, X2
	VMOVQ	X2, 72(DX)
	VPEXTRQ	$1, X2, 72(CX)

	VMOVDQU	10*32(R8), Y0
	VMOVQ	X0, 80(DI)
	VPEXTRQ	$1, X0, 80(SI)
	VEXTRACTI128	$1, Y0, X2
	VMOVQ	X2, 80(DX)
	VPEXTRQ	$1, X2, 80(CX)

	VMOVDQU	11*32(R8), Y0
	VMOVQ	X0, 88(DI)
	VPEXTRQ	$1, X0, 88(SI)
	VEXTRACTI128	$1, Y0, X2
	VMOVQ	X2, 88(DX)
	VPEXTRQ	$1, X2, 88(CX)

	VMOVDQU	12*32(R8), Y0
	VMOVQ	X0, 96(DI)
	VPEXTRQ	$1, X0, 96(SI)
	VEXTRACTI128	$1, Y0, X2
	VMOVQ	X2, 96(DX)
	VPEXTRQ	$1, X2, 96(CX)

	VMOVDQU	13*32(R8), Y0
	VMOVQ	X0, 104(DI)
	VPEXTRQ	$1, X0, 104(SI)
	VEXTRACTI128	$1, Y0, X2
	VMOVQ	X2, 104(DX)
	VPEXTRQ	$1, X2, 104(CX)

	VMOVDQU	14*32(R8), Y0
	VMOVQ	X0, 112(DI)
	VPEXTRQ	$1, X0, 112(SI)
	VEXTRACTI128	$1, Y0, X2
	VMOVQ	X2, 112(DX)
	VPEXTRQ	$1, X2, 112(CX)

	VMOVDQU	15*32(R8), Y0
	VMOVQ	X0, 120(DI)
	VPEXTRQ	$1, X0, 120(SI)
	VEXTRACTI128	$1, Y0, X2
	VMOVQ	X2, 120(DX)
	VPEXTRQ	$1, X2, 120(CX)

	VMOVDQU	16*32(R8), Y0
	VMOVQ	X0, 128(DI)
	VPEXTRQ	$1, X0, 128(SI)
	VEXTRACTI128	$1, Y0, X2
	VMOVQ	X2, 128(DX)
	VPEXTRQ	$1, X2, 128(CX)

	VMOVDQU	17*32(R8), Y0
	VMOVQ	X0, 136(DI)
	VPEXTRQ	$1, X0, 136(SI)
	VEXTRACTI128	$1, Y0, X2
	VMOVQ	X2, 136(DX)
	VPEXTRQ	$1, X2, 136(CX)

	VMOVDQU	18*32(R8), Y0
	VMOVQ	X0, 144(DI)
	VPEXTRQ	$1, X0, 144(SI)
	VEXTRACTI128	$1, Y0, X2
	VMOVQ	X2, 144(DX)
	VPEXTRQ	$1, X2, 144(CX)

	VMOVDQU	19*32(R8), Y0
	VMOVQ	X0, 152(DI)
	VPEXTRQ	$1, X0, 152(SI)
	VEXTRACTI128	$1, Y0, X2
	VMOVQ	X2, 152(DX)
	VPEXTRQ	$1, X2, 152(CX)

	VMOVDQU	20*32(R8), Y0
	VMOVQ	X0, 160(DI)
	VPEXTRQ	$1, X0, 160(SI)
	VEXTRACTI128	$1, Y0, X2
	VMOVQ	X2, 160(DX)
	VPEXTRQ	$1, X2, 160(CX)

	VMOVDQU	21*32(R8), Y0
	VMOVQ	X0, 168(DI)
	VPEXTRQ	$1, X0, 168(SI)
	VEXTRACTI128	$1, Y0, X2
	VMOVQ	X2, 168(DX)
	VPEXTRQ	$1, X2, 168(CX)

	VMOVDQU	22*32(R8), Y0
	VMOVQ	X0, 176(DI)
	VPEXTRQ	$1, X0, 176(SI)
	VEXTRACTI128	$1, Y0, X2
	VMOVQ	X2, 176(DX)
	VPEXTRQ	$1, X2, 176(CX)

	VMOVDQU	23*32(R8), Y0
	VMOVQ	X0, 184(DI)
	VPEXTRQ	$1, X0, 184(SI)
	VEXTRACTI128	$1, Y0, X2
	VMOVQ	X2, 184(DX)
	VPEXTRQ	$1, X2, 184(CX)

	VMOVDQU	24*32(R8), Y0
	VMOVQ	X0, 192(DI)
	VPEXTRQ	$1, X0, 192(SI)
	VEXTRACTI128	$1, Y0, X2
	VMOVQ	X2, 192(DX)
	VPEXTRQ	$1, X2, 192(CX)

	VZEROUPPER
	RET

DATA	round_consts_4x<>+0x000(SB)/8, $0x1
DATA	round_consts_4x<>+0x008(SB)/8, $0x1
DATA	round_consts_4x<>+0x010(SB)/8, $0x1
DATA	round_consts_4x<>+0x018(SB)/8, $0x1
DATA	round_consts_4x<>+0x020(SB)/8, $0x8082
DATA	round_consts_4x<>+0x028(SB)/8, $0x8082
DATA	round_consts_4x<>+0x030(SB)/8, $0x8082
DATA	round_consts_4x<>+0x038(SB)/8, $0x8082
DATA	round_consts_4x<>+0x040(SB)/8, $0x800000000000808a
DATA	round_consts_4x<>+0x048(SB)/8, $0x800000000000808a
DATA	round_consts_4x<>+0x050(SB)/8, $0x800000000000808a
DATA	round_consts_4x<>+0x058(SB)/8, $0x800000000000808a
DATA	round_consts_4x<>+0x060(SB)/8, $0x8000000080008000
DATA	round_consts_4x<>+0x068(SB)/8, $0x8000000080008000
DATA	round_consts_4x<>+0x070(SB)/8, $0x8000000080008000
DATA	round_consts_4x<>+0x078(SB)/8, $0x8000000080008000
DATA	round_consts_4x<>+0x080(SB)/8, $0x808b
DATA	round_consts_4x<>+0x088(SB)/8, $0x808b
DATA	round_consts_4x<>+0x090(SB)/8, $0x808b
DATA	round_consts_4x<>+0x098(SB)/8, $0x808b
DATA	round_consts_4x<>+0x0a0(SB)/8, $0x80000001
DATA	round_consts_4x<>+0x0a8(SB)/8, $0x80000001
DATA	round_consts_4x<>+0x0b0(SB)/8, $0x80000001
DATA	round_consts_4x<>+0x0b8(SB)/8, $0x80000001
DATA	round_consts_4x<>+0x0c0(SB)/8, $0x8000000080008081
DATA	round_consts_4x<>+0x0c8(SB)/8, $0x8000000080008081
DATA	round_consts_4x<>+0x0d0(SB)/8, $0x8000000080008081
DATA	round_consts_4x<>+0x0d8(SB)/8, $0x8000000080008081
DATA	round_consts_4x<>+0x0e0(SB)/8, $0x8000000000008009
DATA	round_consts_4x<>+0x0e8(SB)/8, $0x8000000000008009
DATA	round_consts_4x<>+0x0f0(SB)/8, $0x8000000000008009
DATA	round_consts_4x<>+0x0f8(SB)/8, $0x8000000000008009
DATA	round_consts_4x<>+0x100(SB)/8, $0x8a
DATA	round_consts_4x<>+0x108(SB)/8, $0x8a
DATA	round_consts_4x<>+0x110(SB)/8, $0x8a
DATA	round_consts_4x<>+0x118(SB)/8, $0x8a
DATA	round_consts_4x<>+0x120(SB)/8, $0x88
DATA	round_consts_4x<>+0x128(SB)/8, $0x88
DATA	round_consts_4x<>+0x130(SB)/8, $0x88
DATA	round_consts_4x<>+0x138(SB)/8, $0x88
DATA	round_consts_4x<>+0x140(SB)/8, $0x80008009
DATA	round_consts_4x<>+0x148(SB)/8, $0x80008009
DATA	round_consts_4x<>+0x150(SB)/8, $0x80008009
DATA	round_consts_4x<>+0x158(SB)/8, $0x80008009
DATA	round_consts_4x<>+0x160(SB)/8, $0x8000000a
DATA	round_consts_4x<>+0x168(SB)/8, $0x8000000a
DATA	round_consts_4x<>+0x170(SB)/8, $0x8000000a
DATA	round_consts_4x<>+0x178(SB)/8, $0x8000000a
DATA	round_consts_4x<>+0x180(SB)/8, $0x8000808b
DATA	round_consts_4x<>+0x188(SB)/8, $0x8000808b
DATA	round_consts_4x<>+0x190(SB)/8, $0x8000808b
DATA	round_consts_4x<>+0x198(SB)/8, $0x8000808b
DATA	round_consts_4x<>+0x1a0(SB)/8, $0x800000000000008b
DATA	round_consts_4x<>+0x1a8(SB)/8, $0x800000000000008b
DATA	round_consts_4x<>+0x1b0(SB)/8, $0x800000000000008b
DATA	round_consts_4x<>+0x1b8(SB)/8, $0x800000000000008b
DATA	round_consts_4x<>+0x1c0(SB)/8, $0x8000000000008089
DATA	round_consts_4x<>+0x1c8(SB)/8, $0x8000000000008089
DATA	round_consts_4x<>+0x1d0(SB)/8, $0x8000000000008089
DATA	round_consts_4x<>+0x1d8(SB)/8, $0x8000000000008089
DATA	round_consts_4x<>+0x1e0(SB)/8, $0x8000000000008003
DATA	round_consts_4x<>+0x1e8(SB)/8, $0x8000000000008003
DATA	round_consts_4x<>+0x1f0(SB)/8, $0x8000000000008003
DATA	round_consts_4x<>+0x1f8(SB)/8, $0x8000000000008003
DATA	round_consts_4x<>+0x200(SB)/8, $0x8000000000008002
DATA	round_consts_4x<>+0x208(SB)/8, $0x8000000000008002
DATA	round_consts_4x<>+0x210(SB)/8, $0x8000000000008002
DATA	round_consts_4x<>+0x218(SB)/8, $0x8000000000008002
DATA	round_consts_4x<>+0x220(SB)/8, $0x8000000000000080
DATA	round_consts_4x<>+0x228(SB)/8, $0x8000000000000080
DATA	round_consts_4x<>+0x230(SB)/8, $0x8000000000000080
DATA	round_consts_4x<>+0x238(SB)/8, $0x8000000000000080
DATA	round_consts_4x<>+0x240(SB)/8, $0x800a
DATA	round_consts_4x<>+0x248(SB)/8, $0x800a
DATA	round_consts_4x<>+0x250(SB)/8, $0x800a
DATA	round_consts_4x<>+0x258(SB)/8, $0x800a
DATA	round_consts_4x<>+0x260(SB)/8, $0x800000008000000a
DATA	round_consts_4x<>+0x268(SB)/8, $0x800000008000000a
DATA	round_consts_4x<>+0x270(SB)/8, $0x800000008000000a
DATA	round_consts_4x<>+0x278(SB)/8, $0x800000008000000a
DATA	round_consts_4x<>+0x280(SB)/8, $0x8000000080008081
DATA	round_consts_4x<>+0x288(SB)/8, $0x8000000080008081
DATA	round_consts_4x<>+0x290(SB)/8, $0x8000000080008081
DATA	round_consts_4x<>+0x298(SB)/8, $0x8000000080008081
DATA	round_consts_4x<>+0x2a0(SB)/8, $0x8000000000008080
DATA	round_consts_4x<>+0x2a8(SB)/8, $0x8000000000008080
DATA	round_consts_4x<>+0x2b0(SB)/8, $0x8000000000008080
DATA	round_consts_4x<>+0x2b8(SB)/8, $0x8000000000008080
DATA	round_consts_4x<>+0x2c0(SB)/8, $0x80000001
DATA	round_consts_4x<>+0x2c8(SB)/8, $0x80000001
DATA	round_consts_4x<>+0x2d0(SB)/8, $0x80000001
DATA	round_consts_4x<>+0x2d8(SB)/8, $0x80000001
DATA	round_consts_4x<>+0x2e0(SB)/8, $0x8000000080008008
DATA	round_consts_4x<>+0x2e8(SB)/8, $0x8000000080008008
DATA	round_consts_4x<>+0x2f0(SB)/8, $0x8000000080008008
DATA	round_consts_4x<>+0x2f8(SB)/8, $0x8000000080008008
GLOBL	round_consts_4x<>(SB), NOPTR|RODATA, $768
